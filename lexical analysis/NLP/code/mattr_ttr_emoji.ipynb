{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_bLhzboZGp7"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSDydp7haAIu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import stanza\n",
        "nltk.download('punkt')\n",
        "stanza.download('en')\n",
        "nlp = stanza.Pipeline('en', processors='tokenize,sentiment')\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of8_nW8ZaHvt"
      },
      "outputs": [],
      "source": [
        "filenames = []\n",
        "df = pd.read_csv('/content/drive/MyDrive/original_data/patient_data/patient_info.csv')\n",
        "filenames = list(df['name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDquPNGfbypC"
      },
      "outputs": [],
      "source": [
        "feature_list=[]\n",
        "old_data=[]\n",
        "with open('/content/drive/MyDrive/original_data/patient_time_npl.csv', 'r') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    feature_list=next(csv_reader)\n",
        "    for row in csv_reader:\n",
        "       old_data.append(row)\n",
        "add_feature=['mean_ttr','total_ttr','max_ttr','mean_mattr','total_mattr','max_mattr','mean_brunet_index','total_brunet_index','max_brunet_index','mean_honore_statistic','total_honore_statistic','max_honore_statistic']\n",
        "for i in range(0,len(add_feature)):\n",
        "  feature_list.append(add_feature[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtKPEsF6NrwZ"
      },
      "outputs": [],
      "source": [
        "add_feature=['good_emoji','no_emoji','bad_emoji']\n",
        "for i in range(0,len(add_feature)):\n",
        "  feature_list.append(add_feature[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvMhNq6biLbm"
      },
      "outputs": [],
      "source": [
        "def emoji_analysis(line):\n",
        "  sentence=line.split('.')\n",
        "  good_emoji=0\n",
        "  no_emoji=0\n",
        "  bad_emoji=0\n",
        "  for i in range(0,len(sentence)-1):\n",
        "    try:\n",
        "      doc = nlp(sentence[i])\n",
        "      result=doc.sentences[0].sentiment\n",
        "      if(result==0):\n",
        "        bad_emoji+=1\n",
        "      elif(result==1):\n",
        "        no_emoji+=1\n",
        "      else:\n",
        "        good_emoji+=1\n",
        "    except:\n",
        "      continue\n",
        "  return good_emoji,no_emoji,bad_emoji\n",
        "#emoji_analysis(\"I hate you.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTHqYcEedP0P"
      },
      "outputs": [],
      "source": [
        "def voc_richness(text):\n",
        "  sentences=text.split('.')\n",
        "  ttr=[]\n",
        "  mattr=[]\n",
        "  brunet_index=[]\n",
        "  honore_statistic=[]\n",
        "  for i in range(0,len(sentences)):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    ttr.append(len(set(tokens)) / len(tokens))\n",
        "    def calculate_mattr(tokens, window_size):\n",
        "        ttrs = [len(set(tokens[i:i + window_size])) / window_size for i in range(len(tokens) - window_size + 1)]\n",
        "        return sum(ttrs) / len(ttrs) if len(ttrs) > 0 else 0\n",
        "    mattr.append(calculate_mattr(tokens, window_size=15))\n",
        "    def calculate_brunet_index(total_types, total_tokens):\n",
        "        return total_types / (total_tokens ** 0.165)\n",
        "    def calculate_honore_statistic(total_types, total_tokens):\n",
        "        return 100 * (math.log(total_tokens) / max(1, (1 - (total_types / total_tokens))))\n",
        "    total_types = len(set(tokens))\n",
        "    total_tokens = len(tokens)\n",
        "    brunet_index.append(calculate_brunet_index(total_types, total_tokens))\n",
        "    honore_statistic.append(calculate_honore_statistic(total_types, total_tokens))\n",
        "  return ttr,mattr,brunet_index,honore_statistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pOlN1FZ_eAXI"
      },
      "outputs": [],
      "source": [
        "for i in range(0,len(filenames)):\n",
        "  try:\n",
        "    print(filenames[i])\n",
        "    source_file_path='/content/drive/MyDrive/original_data/ASR/'+filenames[i]+\".txt\"\n",
        "    f1 = open(source_file_path, \"r\")\n",
        "    line1 = f1.readline()\n",
        "    f1.close()\n",
        "    #ttr,mattr,brunet_index,honore_statistic=voc_richness(line1)\n",
        "    good_emoji,no_emoji,bad_emoji=emoji_analysis(line1)\n",
        "    for j in range(0,len(old_data)):\n",
        "      if(old_data[j][0]==filenames[i]):\n",
        "        print(\"complete\",filenames[i])\n",
        "        #old_data[j].append(np.mean(ttr))\n",
        "        #old_data[j].append(np.sum(ttr))\n",
        "        #old_data[j].append(np.max(ttr))\n",
        "        #old_data[j].append(np.mean(mattr))\n",
        "        #old_data[j].append(np.sum(mattr))\n",
        "        #old_data[j].append(np.max(mattr))\n",
        "        #old_data[j].append(np.mean(brunet_index))\n",
        "        #old_data[j].append(np.sum(brunet_index))\n",
        "        #old_data[j].append(np.max(brunet_index))\n",
        "        #old_data[j].append(np.mean(honore_statistic))\n",
        "        #old_data[j].append(np.sum(honore_statistic))\n",
        "        #old_data[j].append(np.max(honore_statistic))\n",
        "        old_data[j].append(good_emoji)\n",
        "        old_data[j].append(no_emoji)\n",
        "        old_data[j].append(bad_emoji)\n",
        "  except:\n",
        "    continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WriAE45HnUuf"
      },
      "outputs": [],
      "source": [
        "for i in range(0,len(old_data)):\n",
        "  all_emoji=sum(old_data[i][feature_list.index('good_emoji'):feature_list.index('bad_emoji')+1])\n",
        "  for j in range(feature_list.index('good_emoji'),feature_list.index('bad_emoji')+1):\n",
        "    if(all_emoji!=0):\n",
        "      old_data[i][j]/=all_emoji\n",
        "    else:\n",
        "      old_data[i][j]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejTJPMoEgg2n"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/original_data/patient_time_npl.csv', 'w', newline='') as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(feature_list)\n",
        "  for i in range(0,len(old_data)):\n",
        "    writer.writerow(old_data[i])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}